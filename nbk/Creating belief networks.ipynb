{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the belief network kitchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Packages we'll need. \"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pyreadstat as prs\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from clean_data_1 import transform_dataframe_1\n",
    "from clean_data_2 import transform_dataframe_2\n",
    "from make_belief_network import make_belief_network\n",
    "from make_belief_network import make_conditional_belief_network\n",
    "from get_basic_graph_info import *\n",
    "from display_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing the GSS dataset. \"\"\"\n",
    "\n",
    "raw_df, meta = prs.read_sas7bdat(\"../dat/gss7222_r3.sas7bdat\")\n",
    "\n",
    "\"\"\" Cleaning the data. \"\"\"\n",
    " \n",
    "df, metadata = transform_dataframe_1(raw_df)    # df contains all our data, metadata contains some other random shit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Setting the core replicating variables that we're interested in. \"\"\"\n",
    "\n",
    "variables = [\"PARTYID\",\"POLVIEWS\",\"NATSPAC\",\"NATENVIR\",\"NATHEAL\",\"NATCITY\",\"NATCRIME\",\"NATDRUG\",\"NATEDUC\",\"NATRACE\",\"NATARMS\",\n",
    "\"NATAID\",\"NATFARE\",\"NATROAD\",\"NATSOC\",\"NATMASS\",\"NATPARK\",\"NATCHLD\",\"NATSCI\",\"EQWLTH\",\"SPKATH\",\"COLATH\",\"LIBATH\",\"SPKRAC\",\"COLRAC\",\"LIBRAC\",\"SPKCOM\",\"COLCOM\",\"LIBCOM\",\"SPKMIL\",\"COLMIL\",\"LIBMIL\",\"SPKHOMO\",\n",
    "\"COLHOMO\",\"LIBHOMO\",\"SPKMSLM\",\"COLMSLM\",\"LIBMSLM\",\"CAPPUN\",\"GUNLAW\",\"COURTS\",\"GRASS\",\"ATTEND\",\"RELITEN\",\"POSTLIFE\",\"PRAYER\",\"AFFRMACT\",\"WRKWAYUP\",\"HELPFUL\",\n",
    "\"FAIR\",\"TRUST\",\"CONFINAN\",\"CONBUS\",\"CONCLERG\",\"CONEDUC\",\"CONFED\",\"CONLABOR\",\"CONPRESS\",\"CONMEDIC\",\"CONTV\",\"CONJUDGE\",\"CONSCI\",\"CONLEGIS\",\"CONARMY\",\"GETAHEAD\",\"FEPOL\",\"ABDEFECT\",\"ABNOMORE\",\"ABHLTH\",\"ABPOOR\",\"ABRAPE\",\"ABSINGLE\",\"ABANY\",\"SEXEDUC\",\"DIVLAW\",\"PREMARSX\",\"TEENSEX\",\"XMARSEX\",\"HOMOSEX\",\"PORNLAW\",\n",
    "\"SPANKING\",\"LETDIE1\",\"SUICIDE1\",\"SUICIDE2\",\"POLHITOK\",\"POLABUSE\",\"POLMURDR\",\"POLESCAP\",\"POLATTAK\",\"NEWS\",\"TVHOURS\",\"FECHLD\",\"FEPRESCH\",\"FEFAM\",\"RACDIF1\",\"RACDIF2\",\"RACDIF3\",\n",
    "\"RACDIF4\",\"HELPPOOR\",\"MARHOMO\", \"PRESLAST_NONCONFORM\", \"PRESLAST_DEMREP\", \"VOTELAST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "NETWORK INFORMATION\n",
      "==================================================\n",
      "\n",
      "Basic Info:\n",
      "  - Size:           102\n",
      "  - Average Degree: 9.88\n",
      "\n",
      "Number of Components: 5\n",
      "\n",
      "Global Network Properties:\n",
      "  - Clustering Coefficient:  0.45\n",
      "  - Average Path Length:     inf\n",
      "  - Network Diameter:        inf\n",
      "\n",
      "Top 5 Nodes by Degree Centrality:\n",
      "  1. HOMOSEX: 0.2970\n",
      "  2. PRESLAST_DEMREP: 0.2970\n",
      "  3. PRAYER: 0.2475\n",
      "  4. POLVIEWS: 0.2376\n",
      "  5. ABANY: 0.2277\n",
      "\n",
      "Top 5 Nodes by Betweenness Centrality:\n",
      "  1. PRESLAST_DEMREP: 0.2238\n",
      "  2. PRAYER: 0.1401\n",
      "  3. HOMOSEX: 0.1036\n",
      "  4. POLHITOK: 0.0993\n",
      "  5. POLVIEWS: 0.0792\n",
      "\n",
      "Top 5 Nodes by Eigenvector Centrality:\n",
      "  1. HOMOSEX: 0.2948\n",
      "  2. ABANY: 0.2599\n",
      "  3. ABNOMORE: 0.2441\n",
      "  4. ABSINGLE: 0.2321\n",
      "  5. GRASS: 0.2255\n",
      "\n",
      "Strongest Correlations:\n",
      "  1. PARTYID <--> PRESLAST_DEMREP (Strength: 0.4099)\n",
      "  2. SPKHOMO <--> COLHOMO (Strength: 0.3838)\n",
      "  3. ABPOOR <--> ABSINGLE (Strength: 0.3404)\n",
      "  4. ATTEND <--> RELITEN (Strength: 0.3402)\n",
      "  5. LETDIE1 <--> SUICIDE1 (Strength: 0.3221)\n",
      "==================================================\n",
      "\n",
      "../out/belief networks/2016-2020, R=0.2, Condition=None/visual.html\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Creating unconditioned belief networks. \"\"\"\n",
    "\n",
    "\"\"\" Belief networks are constructed for a given timeframe and set methodological parameters. \"\"\"\n",
    "\n",
    "# Timeframe - specify the start year and duration of the timeframe\n",
    "start_year = 2016\n",
    "duration = 4\n",
    "timeframe = list(range(start_year, start_year+duration))\n",
    "\n",
    "# Parameters\n",
    "method = \"spearman\"     # method for calculating correlation\n",
    "threshold = 0           # threshold for correlation\n",
    "sample_threshold = 0    # threshold for sample size\n",
    "regularisation = 0.2    # regularisation parameter for partial correlation\n",
    "\n",
    "\"\"\" Note: for now, we keep the threshold and sample threshold at 0. \n",
    "    Regularisation can be set between around 1.5 and 2.5. \"\"\"\n",
    "\n",
    "BN, variables_list, correlation_matrix_partial = make_belief_network(df, variables, timeframe, method=method, is_partial=True, threshold=threshold, \n",
    "                                                                     sample_threshold=sample_threshold, regularisation=regularisation)\n",
    "\n",
    "\"\"\" Print some basic information about the belief network. \"\"\"\n",
    "print_network_info(get_network_info(correlation_matrix_partial, variables_list))\n",
    "\n",
    "\n",
    "\"\"\" Save the graphml, correlation matrix (csv), variables list (csv). \"\"\"\n",
    "save = 1\n",
    "if save:\n",
    "    name = f\"{start_year}-{start_year+duration}, R={regularisation}, Condition=None\"\n",
    "    output_dir = f\"../out/belief networks/{name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    nx.write_graphml(BN, f\"{output_dir}/graph_object.graphml\", named_key_ids=True)\n",
    "    np.savetxt(f\"{output_dir}/correlation_matrix_partial.csv\", correlation_matrix_partial, delimiter=\",\")\n",
    "    np.savetxt(f\"{output_dir}/variables_list.csv\", variables_list, delimiter=\",\", fmt=\"%s\")\n",
    "\n",
    "    net = display_graph_pyvis(BN)\n",
    "    net.show(f\"{output_dir}/visual.html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating conditioned belief networks. \"\"\"\n",
    "\n",
    "# Timeframe - specify the start year and duration of the timeframe\n",
    "start_year = 2000\n",
    "duration = 4\n",
    "timeframe = list(range(start_year, start_year+duration))\n",
    "\n",
    "# Conditioning - specify a list of variables to condition on and a list of corresponding values\n",
    "conditioning = \"PARTYID\"\n",
    "contidion_method = \"negpos\" # \"negpos\" (bins the variable to negative or positive and creates two BNs) or \"unique\" (ceates a BN for each unique value of the variable)\n",
    "\n",
    "# Parameters\n",
    "method = \"spearman\"     # method for calculating correlation\n",
    "threshold = 0           # threshold for correlation\n",
    "sample_threshold = 0    # threshold for sample size\n",
    "regularisation = 0.2    # regularisation parameter for partial correlation\n",
    "\n",
    "\n",
    "outputs = make_conditional_belief_network(conditioning, df, condition_method=contidion_method, variables_of_interest=variables, \n",
    "                                                                                 years_of_interest=timeframe, method=method, is_partial=True, threshold=threshold, \n",
    "                                                                                 sample_threshold=sample_threshold, regularisation=regularisation)\n",
    "\n",
    "# conditioned_BN, conditioned_variables_list, conditioned_correlation_matrix_partial \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
