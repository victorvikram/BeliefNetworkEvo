## Introduction
If you're at a restaurant and there are $2$ appetizers, $2$ mains, and $2$ desserts, how many different possible meals are there? A high school combinatorics class will give you the answer: $2 \times 2 \times 2 = 2^3 = 8$. Similarly, there are many ($n$) issues of importance to American voters (these are the "courses" in the analogy), and on each issue you could take one of several ($k$) positions (these are the dishes). Then the number of policy platforms that should in theory be possible is $k^n$. Whenever $k > 1$ and $n > 2$, then, there will be more than two possible platforms. Yet, we have a two-party system. How does this happen?

When opinions get associated with each other, it effectively reduces the number of policy platforms you can have. For instance, if we know that people who get the salad as an appetizer always get the vegetarian curry as the main course, the $(\text{salad}, \text{steak}, \text{tiramisu})$ and $(\text{salad}, \text{steak}, \text{panna cotta})$ meals never happen. This reduces the number of possible meals to six. In a similar way, if people who support abortion rights always also support gay marriage, you eliminate all the platforms that simultaneously support abortion but oppose gay marriage — of which there are $k^{n - 2}$. 

Sometimes we will call stances on particular issues first-order beliefs (say, whether you support more healthcare spending), and correlations between beliefs second-order beliefs  (whether a stance on healthcare spending and abortion rights are positively or negative correlated). Note that the way we are measuring second-order beliefs, they are properties of *groups*, not individuals — the correlations are calculated across many individuals. This is not to say that one couldn't conceive of a notion of second-order belief for an individual, if there was a way to ascertain that two beliefs were linked in a single person's head. However, for our purposes, we are interested in constraints on the distribution of beliefs of the entire collective, and for this, group-level correlations are enough.

In the extreme case where each opinion only has two possible stances (let us say, pro or con), and every issue is associated with every other issue (e.g., being pro-choice means you are also pro-universal healthcare and also against foreign military intervention, etc.), we would arrive at a case where there are two possible platforms. In this world, it would be clear what platforms the two parties should adopt: one of the two possible options. This, however, is not the world we live in now. People are not so tightly constrained in there views, and as a result, there is a greater heterogeneity of possible platforms to choose from. The task, then, of picking a platform becomes much more challenging, and evidence shows that the parties have not met with resounding success: [70% of Americans wish there were more parties to choose from](https://www.pewresearch.org/politics/2022/08/09/as-partisan-hostility-grows-signs-of-frustration-with-the-two-party-system/). 

This paper analyzes the associations that exist between beliefs as a way to understand how exactly beliefs are constrained. This will help us answer a number of vitally important questions:
- Which platforms are possible that respect the observed associations between beliefs?
- How have belief associations changed from the 1970s to the present?
- Do liberals and conservatives exhibit only first-order disagreement — disagreement on stances of particular beliefs — or do they also have second-order disagreement — disagreement on the associations between beliefs?

It is worth taking brief stock of this paper's ontology. A (first-order) belief, for our purposes, will be a stance on a particular issue (e.g., $\text{pro-choice}$). A belief vector is a sequence of numbers indicating a particular set of beliefs that a person holds, (e.g., $[\text{pro-choice}, \text{fiscally conservative}, \text{isolationist}]$). A belief space is then an $n$-dimensional space where points are particular belief vectors. For instance, in the last example the belief space is a three dimensional space where the $x$ axis is the spectrum from completely pro-life to completely pro-choice, the $y$ axis is the spectrum from fiscally liberal to fiscally conservative, and the $z$ axis is the spectrum from interventionist to isolationist. We can imagine many points in this space, representing the beliefs of each person in the population.

Saying that beliefs are *constrained* means that they are *correlations* between them, where we are using correlation in its general sense: an informational association between two variables (knowing something about $x$ teaches you something about what $y$ is likely to be). What is the link between *constraint* and *correlation*? Well, if there are no informational associations between the different beliefs (they are independent), then knowing $x$ doesn't close off any values of $y$ or $z$. The space remains maximally unconstrained. By contrast, if there is an informational association between $x$ and $y$, then knowing $x$ *does* teach us something about where $y$ is likely, and not likely, to be. This disqualifies some possible values of $y$, or at least, make them less likely, thereby constraining where belief vectors are likely to be in the space. In this way, correlations are evidence of constraints in the space of possible beliefs.

Sometimes, especially in high dimensional spaces, it can be difficult to see these correlations. This is why we use *belief networks*, which can capture some of these interdependencies (the pairwise ones) in an easy-to visualize way. In belief networks, the nodes are (first-order) beliefs, and the edges are correlations between the connected beliefs (correlations being, in general terms, when knowing one variable tells you information about another). 
## Method
In order to build a belief network and answer our questions, we must choose a way to measure these correlations. We used regularized partial spearman correlations to do this. Why this method? I will justify each of the three "modifiers" (regularized, partial, and spearman) from back to front. 
### Spearman
Spearman correlations do not capture *all* associations between variables, rather, tehy capture the relationship of the sort "if $x$ is higher, $y$ tends to be higher too". A *perfect* spearman correlation of 1 occurs when the relationship between $x$ and $y$ is monotonic: if $x$ is higher, $y$ is higher. We use spearman correlations like this for two reasons. First, our data is ordinal, in that the survey questions are ranked on a discrete scale (disagree = 1, neutral = 2, agree = 3, for instance). Because of this, the exact numerical values of the variables is not meaningful: to say that neutral is "two times" disagree is meaningless. What matters is the order: that, for instance, "neutral" is closer to "agree" than to disagree. For this reason, it makes sense to measure correlations on the *rank order*, and not the *exact values*, of the measurements. This is what spearman correlation does: it is simply the classic pearson correlation done on the rank order of the measurements. Because the spearman correlation is on the *rank*, it doesn't "care" about the exact functional form of the relation. Whether the functional relation be linear, exponential, or any other monotonic function, the ranks will look the same: high values of $x$ will pair with high values of $y$. 

Still, spearman correlations only pick up on monotonic relationships between variables. If there was a "u-curve" relationship between variables, where, for instance, extreme conservatives and extreme liberals both support import tariffs, the method would not pick up on the link between the variables. While this is admittedly an imperfection in the method, we are ok with it because we have observed that when two variables do appear linked, usually the relationship can be approximated as something monotonic [need to make a stronger case for this]. Moreover, a more complex analysis of the associations between variables would make the analysis much less transparent, and we opted for something simple and easy to understand. 

### Partial
If A is correlated with B, and B with C, then A will likely be correlated with C. This means that the correlation network will be fully connected. When there are more than just three issues, this situation can lead to the presence of many edges. Calculating partial correlations reduces this density of edges. This is because there is a higher bar for a partial correlation to exist: two variables $x$ and $y$ are partially correlated if they *still* have a linear relationship even after accounting for the linear effects of all the other variables. In other words, $x$ has (linear) predictive power over $y$ over and above the (linear) effects of the other variables. In the case of our initial example, then, perhaps A is correlated with C, but it might not provide any predictive power over and above what B can already provide. Therefore, there would be no edge between A and C.  

This is important because it means that in using partial correlations, associations are counted only one time. In the above example, when the association between A and C is fully explainable by the A-B and B-C correlations, having an A-C edge would simply add clutter without giving us any new information. By taking partial correlations, we are guaranteed that every edge does give us new information.

### Regularized
Regularization is used in optimization algorithms to penalize higher parameter values (in our case, the estimates of the partial correlations). The justification for this is the intuition that any particular simple model (one with fewer parameters) is "more likely" than a complex model. Imagine a convoluted conspiracy theory compared to a simple explanation: we should a priori expect the simple explanation to be more likely, since fewer things have to "go right" for the simple explanation to be correct. Regularization simply takes this prior bias towards a simpler model into account. 

Another way to put it is in terms of hypothesis testing. In science, the null hypothesis is usually that there is *no association* between two variables, and the onus is on the scientist is to provide evidence otherwise. What regularization does, by pushing small edges to zero, is it eliminates the edges we are not so sure about, retaining only the edges that we are more confident in. This, too, can be interpreted as a bias towards simple models (the null hypothesis model is simply the one where the variables vary independently, which is simpler than the one where the variables covary in some way). 

The reason we use absolute value (lasso) regularization instead of squared regularization is that it has a stronger sparsifying effect. Squared values of values $<1$  (in contrast to values $>1$) are very small, so there is not that much benefit to pushing them to 0. With absolute value, on the other hand, pushing $0.05$ to $0$ provides as much of a benefit as pushing $3.05$ to $3$. 
### Note on causality
There is of course the well worn adage that correlation doesn't imply causation, and I should argue why for our purposes this is not an issue. On the level of individual views, if they are associated, it could be a result of one . Taking a more global picture, it is not clear if genuine interactions between beliefs disqualify certain regions of the space as a result of cognitive dissonance. For instance, perhaps simultaneously believing in free trade and restricted immigration would cause dissonance, as one view implicitly endorses free movement while the other curtails it. In this case the association between these two beliefs plays a causal role in confining people to subsets of the belief space. But in another case, where, for instance, being raised Republican leads one to adopt the views conventionally associated with republicans, or being a member of a group with a certain set of interests makes one adopt a consortium of views favorable to those interests, the associations emerge out of constraints not observed in the network of beliefs itself. 

Nevertheless, we can say that the existence of associations is evidence of some constraint existing. Either the association itself is an interaction, and it actively constrains the belief space, or there is some other dynamic that constrains the belief space, leading to beliefs getting associated. So while the method does not allow us to say what exactly the constraints are, it does allow us to see evidence of a constraint at work. 


## Questions that may belong in another narrative
- Have belief associations remained relatively static as our stances on beliefs have changed, or do associations and beliefs co-evolve? If the latter, are their patterns in the way our first- and second-order beliefs co-evolve? 


